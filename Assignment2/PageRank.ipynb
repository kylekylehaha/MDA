{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark import SparkConf, SparkContext\n",
    "os.environ['JAVA_HOME'] = 'C:\\Program Files\\Java\\jdk1.8.0_301'\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(i, j)$ pair 代表存在有向邊 $i \\rightarrow j$\n",
    "\n",
    "在 `mapper1` 中，讀取 `input.txt` 形成 $(i, j)$ pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper1(line):\n",
    "    line = line.split()\n",
    "    maplist = []\n",
    "    maplist.append((line[0], line[1]))\n",
    "\n",
    "    return maplist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 `mapper2` 中，`x[0]` 為 node $i$，而 `x[1][0]` 為所有 node $i$ 的 out-linker。\n",
    "\n",
    "故在數學式$r_j=\\displaystyle\\sum_{i\\rightarrow j}\\beta\\frac{r_i}{d_i}+(1-\\beta)\\frac{1}{N}$ 中的 $d_i$ 即為 `len(x[1][0])`。\n",
    "\n",
    "計算時，因為 $\\beta$ 與 i 無關，因此我們將 $\\beta$ 提出來，之後再運算。\n",
    "\n",
    "\n",
    "\n",
    "在 `mapper2` 中，我們計算 $\\frac{r_i}{d_i}$，並形成 $(j, \\frac{r_i}{d_i}$) 的 pair，最後再利用 `reduceByKey` 來算出每一項的 $r_j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper2(x):\n",
    "    ranks_list = []\n",
    "\n",
    "    # x[0] 為 node i\n",
    "    # x[1][0] 為所有 node i 射出的點，故 len(x[1][0]) 為 out-degree of node i\n",
    "    for j in x[1][0]:\n",
    "        # r_new = beta * (float(x[1][1])/len(x[1][0]))\n",
    "        r_new = float(x[1][1])/len(x[1][0])\n",
    "\n",
    "        # 我們最後是以 j 為 reduceByKey 的 key, 故這邊我們要 append((j, r_new))\n",
    "        ranks_list.append((j, r_new))\n",
    "\n",
    "    return ranks_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 `count` 中，因為有些點只有 in-link，沒有 out-link，故在計算 total node number $N$ 時會少算。\n",
    "\n",
    "這邊我們重新讀檔，將所有的 node 加入 `node_list`，因為所有 node 皆是有編號的，因此我們利用最大的編號數來代表 total node number $N$ 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(line):\n",
    "    node_list = []\n",
    "    line = line.split()\n",
    "\n",
    "    # Add node into node_list\n",
    "    # 若重複出現者，則不用加入\n",
    "    if int(line[0]) not in node_list:\n",
    "        node_list.append(int(line[0]))\n",
    "    \n",
    "    if int(line[1]) not in node_list:\n",
    "        node_list.append(int(line[1]))\n",
    "    \n",
    "    return node_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducer(x,y):\n",
    "    return x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(res_list):\n",
    "    f = open('Outputfile.txt', 'w')\n",
    "    str = \"{}\\t{}\\n\"\n",
    "\n",
    "    out_cnt = len(res_list)\n",
    "    if out_cnt > 10:\n",
    "        out_cnt = 10\n",
    "    for i in range(out_cnt):\n",
    "        f.write(str.format(res_list[i][1], round(res_list[i][0], 6)))\n",
    "        print(str.format(res_list[i][1], round(res_list[i][0], 6)))\n",
    "    print(\"Done\")\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster(\"local\").setAppName(\"PageRank\")\n",
    "sc = SparkContext(conf=conf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "經過 `mapper1` 後形成 $(i, j)$ pair。\n",
    "\n",
    "透過 `groupByKey()` 形成 (node i, List of node's outlink)。\n",
    "\n",
    "需注意 node 編號從 0 開始，因此最後 total node number 需要再 $+1$\n",
    "\n",
    "一開始， $\\forall r_j, \\; r_j=1/N$，因此我們 `ranks` 採用 $(j, r_j)$ pair。\n",
    "\n",
    "> 需注意因為 `input-test.txt` 的 node 編號從 1 開始，因此 `ranks` 的初始值應為 (i+1, 1/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10879\n"
     ]
    }
   ],
   "source": [
    "links = sc.textFile(\"input.txt\")\n",
    "links = links.flatMap(mapper1)\n",
    "\n",
    "# Calculate all page\n",
    "# 如果有個點沒有 out-link，則不會算到!!\n",
    "# links 型式: (page, list of page's outlink)\n",
    "links = links.groupByKey()\n",
    "\n",
    "N = sc.textFile(\"input.txt\")\n",
    "N = N.flatMap(count).max()\n",
    "# node num start from 0\n",
    "N = int(N)+1\n",
    "\n",
    "# Create and initialize the ranks\n",
    "ranks = sc.parallelize([(str(i), 1/N) for i in range(N)])\n",
    "# # 若 input 為 `input-test.txt`，node 編號從 1 開始，因此需注意 ranks 初始值\n",
    "# ranks = sc.parallelize([(str(i+1), 1/N) for i in range(N)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們透過 `.join()` 形成 (node i, [[list of i's out-link], $r_j$]) 的 format。\n",
    "\n",
    "經過 `mapper2` 和 `reducer` 後，即完成數學式 $r_j=\\displaystyle\\sum_{i\\rightarrow j}\\beta\\frac{r_i}{d_i}+(1-\\beta)\\frac{1}{N}$ 中的 $\\displaystyle\\sum_{i\\rightarrow j}\\frac{r_i}{d_i}$。\n",
    "\n",
    "接著，需乘上 $\\beta$ 和後面的 $\\frac{(1-\\beta)}{N}$，形成$r^{new}_j$\n",
    "\n",
    "我們透過 $\\sum_j r^{new}_j \\lt 1$ 來判斷是否出現 **dead-end**。 若我們發現 **dead-end** 的出現，則必須 renormalize:\n",
    "$$\n",
    "\\forall j: r^{new}_j=r'^{new}_j+\\frac{1-S}{N}\n",
    "$$\n",
    ", where $S=\\sum r'^{new}_j$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10861\t0.00063\n",
      "\n",
      "4240\t0.00063\n",
      "\n",
      "6899\t0.000526\n",
      "\n",
      "9526\t0.000513\n",
      "\n",
      "2118\t0.000497\n",
      "\n",
      "3419\t0.000486\n",
      "\n",
      "1311\t0.000481\n",
      "\n",
      "3186\t0.000472\n",
      "\n",
      "3541\t0.000464\n",
      "\n",
      "367\t0.000462\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "Iteration = 20\n",
    "beta = 0.8\n",
    "constant = (1-beta)/N\n",
    "\n",
    "for i in range (Iteration):\n",
    "    ranks = links.join(ranks).flatMap(mapper2).reduceByKey(reducer)\n",
    "    ranks = ranks.map(lambda x: (x[0], beta*x[1]+constant))\n",
    "\n",
    "    # Detect whether dead-end occur\n",
    "    ranks_sum = ranks.map(lambda x: x[1])\n",
    "    s = ranks_sum.sum()\n",
    "    if s < 1:\n",
    "        # print(\"Detect Dead-end\")\n",
    "        ranks = ranks.map(lambda x: (x[0], x[1]+(1-s)/N))\n",
    "        \n",
    "# 將 key value 對換，這樣可以利用 sortByKey 來達成 output format\n",
    "ranks = ranks.map(lambda x : (x[1], x[0]))\n",
    "output(ranks.sortByKey(False).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
